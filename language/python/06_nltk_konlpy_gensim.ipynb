{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "7e629cece19eff703506f3e72ba025fb9f1dd6fcb2d004c453944190b24358da"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c6d7297c38c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from konlpy.tag import Kkma, Okt, Mecab\n",
    "\n",
    "okt=Okt()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'TreebankWordTokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-44e169cce332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TreebankWordTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
    "\n",
    "sent_tokenize(text)\n",
    "\"\"\"\n",
    "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n",
    "\"\"\"\n",
    "tokenizer.tokenize(text)\n",
    "\"\"\"\n",
    "['His', 'barber', 'kept', 'his', 'word.', 'But', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'him', 'crazy.', 'Finally', ',', 'the', 'barber', 'went', 'up', 'a', 'mountain', 'and', 'almost', 'to', 'the', 'edge', 'of', 'a', 'cliff.', 'He', 'dug', 'a', 'hole', 'in', 'the', 'midst', 'of', 'some', 'reeds.', 'He', 'looked', 'about', ',', 'to', 'mae', 'sure', 'no', 'one', 'was', 'near', '.']\n",
    "\"\"\"\n",
    "okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\") \n",
    "\"\"\"\n",
    "# morpheme tokenization\n",
    "\"\"\"\n",
    "print(kkma.nouns(u'질문이나 건의사항은 깃헙 이슈 트래커에 남겨주세요.'))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "print(kkma.pos(u'오류보고는 실행환경, 에러메세지와함께 설명을 최대한상세히!^^'))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "train = ['call you tonight', 'Call me a cab', 'Please call me... PLEASE!']\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "count_vect.fit(train)\n",
    "tfidf_vect.fit(train)\n",
    "ngram_vect.fit(train)\n",
    "\n",
    "count_vect.get_feature_names() # ['cab', 'call', 'me', 'please', 'tonight', 'you']\n",
    "ngram_vect.get_feature_names() # unigram get_feature_names        ['cab', 'call', 'me', 'please', 'tonight', 'you'] \n",
    "\n",
    "count_dtm = count_vect.transform(train)\n",
    "ngram_dtm = ngram_vect.transform(train)\n",
    "tfidf_dtm = tfidf_vect.transform(train)\n",
    "\n",
    "pd.DataFrame(count_dtm.toarray(), columns=count_vect.get_feature_names())  \n",
    "\"\"\"\n",
    "# one row per document and one column per token\n",
    "cab\tcall\tme\tplease\ttonight\tyou\n",
    "0\t0\t1\t0\t0\t1\t1\n",
    "1\t1\t1\t1\t0\t0\t0\n",
    "2\t0\t1\t1\t2\t0\t0\n",
    "\"\"\"\n",
    "pd.DataFrame(ngram_dtm.toarray(), columns=ngram_vect.get_feature_names())\n",
    "\"\"\"\n",
    "call me\tcall you\tme cab\tme please\tplease call\tyou tonight\n",
    "0\t0\t1\t0\t0\t0\t1\n",
    "1\t1\t0\t1\t0\t0\t0\n",
    "2\t1\t0\t0\t1\t1\t0\n",
    "\"\"\"\n",
    "pd.DataFrame(tfidf_dtm.toarray(), columns=tfidf_vect.get_feature_names())\n",
    "\"\"\"\n",
    "cab\tcall\tme\tplease\ttonight\tyou\n",
    "0\t0.000000\t0.385372\t0.000000\t0.000000\t0.652491\t0.652491\n",
    "1\t0.720333\t0.425441\t0.547832\t0.000000\t0.000000\t0.000000\n",
    "2\t0.000000\t0.266075\t0.342620\t0.901008\t0.000000\t0.000000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}